# Terminologies and definitions of parallel computing
- Core: a single execution unit that has a private L1 cache and can execute instructions independently.
- Processor: several cores on a single Integrated Circuit (IC) or a chip called a multi-core processor. 
- Socket: physical connector into which an IC/chip or processor is inserted.
- Node: a packaging of sockets - motherboard or printed circuit board (PCB) that has multiple sockets.

# Job Scheduling
- HPC systems use job or batch scheduling.
- Each user submits their parallel programs for execution to a "job" scheduler.
- The scheduler decides:
	- What job to schedule next.
	- What resources to allocate to the jobs.

# Compute Nodes vs Login Nodes
- Compute nodes: dedicated notes for running jobs.
	- Can only be accessed when they have been allocated to a user by the job scheduler.
- Login nodes: nodes shared by all users to compile their programs, submit jobs, etc.
- Service/management nodes for IO, etc.

# Supercomputers vs. Commodity Clusters
- Supercomputer refers to a large expensive installation, typically using custom hardware.
	- High-speed interconnect.
	- IBM Blue Gene, Cray XT, Cray XC
- Cluster refers to a cluster of nodes, typically put together using commodity (COTS) hardware.

# Serial vs. Parallel Code
- Thread: a thread or path of execution managed by the operating system (OS)
	- Threads share the same memory address space.
- Process: heavy-weight, processes do not share resources such as memory, file descriptors, etc.
- Serial or sequential code: can only run on a single thread or process.
- Parallel code: can be run on one or more threads or processes.

# Scaling and Scalable
- Scaling: the action of running a parallel program on 1 to N processes.
	- 1, 2, 3, 4, ..., n
	- 1, 2, 4, 8, ..., n
- Scalable: A program is *scalable* if its performance improves when using more resources.

# Weak vs. Strong Scaling
- Strong scaling: *Fixed total* problem size as we run on more resources (processes or threads)
	- Sorting numbers on 1 process, 2 processes, 4 processes...
- Weaking scaling: Fixed problem per process but *increasing total* problem size as we run on more resources.
	- Sorting n numbers on 1 process.
	- 2n numbers on 2 processes.
	- 4n numbers on 4 processes.

# Speedup and Efficiency
- (Parallel) Speedup: Ratio of execution time on one process to that on p processes.
$$ 
  Speedup = \frac{t_1}{t_p}
  $$
- (Parallel) Efficiency: Speedup per process
  $$
	  Efficiency = \frac{t_1}{t_p \times p}
   $$
# Amdahl's Law
- Speedup is limited by the serial portion of the code 
	- Often referred to as the serial "bottleneck" -- the portion that cannot be serialized.
- Let's say only a fraction *f* of the program (in terms of execution time) can be parallelized on *p* processes.
  $$
	  Speedup = \frac{1}{(1-f)+f/p}
  $$
# Communication and Synchronization
- Each process may execute serial code independently for a while
- When data is needed from other (remote) processes, message is required.
	- Referred to as communication or synchronization (or MPI messages)
- Intra-node communication: among cores within a node.
- Inter-node communication: among cores on different nodes connected by a network.
- Bulk synchronous programs: all processes compute simultaneously, then synchronize (communicate) together.

# Different Models of Parallel Computation
- SISD: Single instruction Single Data.
- SIMD: Single Instruction Multiple Data.
	- GPUs
- MIMD: Multiple Instruction Multiple Data.
	- Two variations:
		- SIMT: Single Instruction Multiple Threads
			- Threads execute in lock-step
			- GPUs.
		- SPMD: Single Program Multiple Data
			- All processes execute the same program, but act on different data.
			- Enables MIMD parallelization.
- https://en.wikipedia.org/wiki/Flynn's_taxonomy

