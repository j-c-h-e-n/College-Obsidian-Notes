- An agent that can plan ahead (to consider a sequence of actions that form a path to a goal state) is called a problem-solving agent.
	- The computation it undertakes is called a search.
	- Representations are atomic (nodes).
- Agents that use factored or structured representations of states are called planning agents (discussed later in Ch 7 and 11).
- This chapter covers searches in the simplest environment: episodic, single agent, fully observable, deterministic, static, discrete, and known.
- Informed can estimate their progress towards the goal while non-informed algorithms do not have this estimating ability.
## 3.1 Problem-Solving Agents
 - 4 phase problem solving process:
	 - Goal formulation: Agent adopting a goal. Goals allow behavior regulation through objective limitation which limits candidate actions.
	- Problem formulation: Agent creates a description of the states and actions needed to reach their goal.
	- Search: Agent simulates sequences of actions in its model until it finds a sequence of actions that reaches the goal, which is called a solution.
	- Execution: Agent can now follow through its solution, executing actions one at a time.
- In any fully observable, deterministic, known environment, the solution to any problem is a fixed sequence of actions.
	- Agents can ignore percepts in a deterministic environment because the solution is guaranteed to reach the goal. This is called an open-loop system.
	- If there is a chance the model can break or the environment is nondeterministic, then the agent should use their percepts. This is called a closed-loop system.
- Partially observable or nondeterministic environments (requires the agent to use their percepts) will result in a solution that uses a branching strategy that can recommend different future actions depending on what their percepts reveal.
### 3.1.1 Search problems and solutions
- Search problems are defined formally as follows:
	- A set of possible states that the environment can be in. This is called the state space.
	- The initial state that the agent starts in.
	- A set of one or more goal states. Sometimes there are multiple goal states.
	- The actions available to the agent. Further, the set of actions available to an agent in specific states.
	- A transition model that describes what each action does (the details, cause/effect of actions).
	- An action cost function that applies a cost to each action in state s to state s'.
		- Cost functions should be reflective of the agent's performance measure.
- More definitions:
	- A sequence of actions is called a path. A solution is a path from the initial state to a goal state.
	- The optimal solution is the solution with the lowest path cost amongst all solutions.
- The state space/environment can be represented with a graph where vertices (nodes) are states and edges are actions.
### 3.1.2 Formulating problems
- The formulation of a problem is called a model, an abstract mathematical description of the real thing.
	- The process of creating a model heavily relies on abstraction to remove as much detail as possible to simplify our environment.
- The best abstractions involve "removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out".
	- Without this ability, intelligent agents will get swamped by the real world.
## 3.2 Example Problems
- Standardized problems: intended to illustrate or exercise various problem-solving methods. Used as a benchmark for algorithm performance.
- Real-world problems: problems that people actually encounter, solutions that people will use, and formulations that are idiosyncratic due to the varying designs of robots and problem situations.
### 3.2.1 Standardized problems
- Grid World: a 2D rectangular array of square cells where agents can move from cell to cell. Cells can contain obstacles and/or objects that agents can interact with.
	- Vacuum world where the vacuum agent can only move left or right and suck dirt.
	- Sokoban puzzle where the agent's goal is to push boxes into designated locations.
- Sliding-tile puzzle similar to the board game/puzzle Rush Hour. Tiles can only slide to blank spaces and the goal is to achieve a specific pattern.
	- The best known is the 8-puzzle which is a 3x3 grid with 8 numbered tiles and 1 blank tile.
- A mathematical problem that shows how infinite state spaces can arise.
	- The theory behind this is that the number 4 can achieve any desired positive integer through a sequence of square roots, floors, and factorials.
		- Ex: $\lfloor \sqrt{\sqrt{\sqrt{\sqrt{\sqrt{(4!)!}}}}} \rfloor = 5$
### 3.2.2 Real-world problems
- Search problems/route-finding problems are defined in terms of specific locations and transitions along edges between them (see 3.1.1).
	- Applications involve car navigation while driving, network routing, military operations planning, airline travel-planning, etc.
- Touring problems describe a set of locations that must be visited instead of a single goal destination.
	- The Traveling Salesperson Problem (TSP) is a problem where every city on a map must be visited. The optimal solution is the path/tour that results in the lowest cost possible.
- A VLSI layout problem deals with optimizing positions of componentry and connections on a chip to minimize area, which in turn minimizes circuit delays, stray capacitances, and maximizes manufacturing yield.
- Robot navigation is a generalization of the route-finding problem. This problem allows the robot to roam freely, creating its own paths. The robot also has limbs that must be controlled, resulting in the search space being multi-dimensional. The complexity is furthered by the inclusion of sensors, motors, and environmental unpredictability.
## 3.3 Search Algorithms
- A search algorithm takes a search problem as input and either returns a solution or an indication of failure.
- This chapter dives into algorithms that superimpose a search tree over the state-space graph.
	- Important distinction: state-space represents the set of states and actions that connect them in the world. Search trees simply represents the path between these states that reaches the goal.
### 3.3.1 Best-first search
- Super general greedy approach.
- The next node $n$ is chosen from the evaluation function $f(n)$. The node on the frontier/fringe  corresponding to the minimum value is chosen for expansion.
	- Each child node is then added to the frontier if it has not been reached before. If reached before, it is now re-added if the path cost is less than previously.
### 3.3.2 Search data structures
- Search algorithms require a data structure to keep track of the search tree.
- The most basic data structure is a node, represented with 4 components:
	- node.state: the state to which the node corresponds.
	- node.parent: the node in the tree that generated this node.
	- node.action: the action that was applied to the parent's state to generate this node.
	- node.path-cost: the total cost of the path from the initial state to this node. $g(node)$ is a synonym for path-cost.
- We also need a data structure to store the frontier/fringe. Most of the time, the best is a queue. The frontier usually needs the following functions:
	- An empty check.
	- A pop (remove and returns).
	- A way to retrieve (not remove) the most recently added.
	- A way to add more elements.
- Three kinds of queues are used:
	- Priority queue: Elements are sorted by minimum cost, pop returns and removes the least cost element.
	- FIFO (First-in-First-out): Pop returns and removes the first element that was added to the queue. Usually used in BFS.
	- LIFO (Last-in-First-out): AKA a stack. Pop returns and removes the most recently added element. Usually used in DFS.
### 3.3.3 Redundant paths
