- An agent that can plan ahead (to consider a sequence of actions that form a path to a goal state) is called a problem-solving agent.
	- The computation it undertakes is called a search.
	- Representations are atomic (nodes).
- Agents that use factored or structured representations of states are called planning agents (discussed later in Ch 7 and 11).
- This chapter covers searches in the simplest environment: episodic, single agent, fully observable, deterministic, static, discrete, and known.
- Informed can estimate their progress towards the goal while non-informed algorithms do not have this estimating ability.
## 3.1 Problem-Solving Agents
 - 4 phase problem solving process:
	 - Goal formulation: Agent adopting a goal. Goals allow behavior regulation through objective limitation which limits candidate actions.
	- Problem formulation: Agent creates a description of the states and actions needed to reach their goal.
	- Search: Agent simulates sequences of actions in its model until it finds a sequence of actions that reaches the goal, which is called a solution.
	- Execution: Agent can now follow through its solution, executing actions one at a time.
- In any fully observable, deterministic, known environment, the solution to any problem is a fixed sequence of actions.
	- Agents can ignore percepts in a deterministic environment because the solution is guaranteed to reach the goal. This is called an open-loop system.
	- If there is a chance the model can break or the environment is nondeterministic, then the agent should use their percepts. This is called a closed-loop system.
- Partially observable or nondeterministic environments (requires the agent to use their percepts) will result in a solution that uses a branching strategy that can recommend different future actions depending on what their percepts reveal.
### 3.1.1 Search problems and solutions
- Search problems are defined formally as follows:
	- A set of possible states that the environment can be in. This is called the state space.
	- The initial state that the agent starts in.
	- A set of one or more goal states. Sometimes there are multiple goal states.
	- The actions available to the agent. Further, the set of actions available to an agent in specific states.
	- A transition model that describes what each action does (the details, cause/effect of actions).
	- An action cost function that applies a cost to each action in state s to state s'.
		- Cost functions should be reflective of the agent's performance measure.
- More definitions:
	- A sequence of actions is called a path. A solution is a path from the initial state to a goal state.
	- The optimal solution is the solution with the lowest path cost amongst all solutions.
- The state space/environment can be represented with a graph where vertices (nodes) are states and edges are actions.
### 3.1.2 Formulating problems
- The formulation of a problem is called a model, an abstract mathematical description of the real thing.
	- The process of creating a model heavily relies on abstraction to remove as much detail as possible to simplify our environment.
- The best abstractions involve "removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out".
	- Without this ability, intelligent agents will get swamped by the real world.
## 3.2 Example Problems
- Standardized problems: intended to illustrate or exercise various problem-solving methods. Used as a benchmark for algorithm performance.
- Real-world problems: problems that people actually encounter, solutions that people will use, and formulations that are idiosyncratic due to the varying designs of robots and problem situations.
### 3.2.1 Standardized problems
- Grid World: a 2D rectangular array of square cells where agents can move from cell to cell. Cells can contain obstacles and/or objects that agents can interact with.
	- Vacuum world where the vacuum agent can only move left or right and suck dirt.
	- Sokoban puzzle where the agent's goal is to push boxes into designated locations.
- Sliding-tile puzzle similar to the board game/puzzle Rush Hour. Tiles can only slide to blank spaces and the goal is to achieve a specific pattern.
	- The best known is the 8-puzzle which is a 3x3 grid with 8 numbered tiles and 1 blank tile.
- A mathematical problem that shows how infinite state spaces can arise.
	- The theory behind this is that the number 4 can achieve any desired positive integer through a sequence of square roots, floors, and factorials.
		- Ex: $\lfloor \sqrt{\sqrt{\sqrt{\sqrt{\sqrt{(4!)!}}}}} \rfloor = 5$
### 3.2.2 Real-world problems
- Search problems/route-finding problems are defined in terms of specific locations and transitions along edges between them (see 3.1.1).
	- Applications involve car navigation while driving, network routing, military operations planning, airline travel-planning, etc.
- Touring problems describe a set of locations that must be visited instead of a single goal destination.
	- The Traveling Salesperson Problem (TSP) is a problem where every city on a map must be visited. The optimal solution is the path/tour that results in the lowest cost possible.
- A VLSI layout problem deals with optimizing positions of componentry and connections on a chip to minimize area, which in turn minimizes circuit delays, stray capacitances, and maximizes manufacturing yield.
- Robot navigation is a generalization of the route-finding problem. This problem allows the robot to roam freely, creating its own paths. The robot also has limbs that must be controlled, resulting in the search space being multi-dimensional. The complexity is furthered by the inclusion of sensors, motors, and environmental unpredictability.
## 3.3 Search Algorithms
- A search algorithm takes a search problem as input and either returns a solution or an indication of failure.
- This chapter dives into algorithms that superimpose a search tree over the state-space graph.
	- Important distinction: state-space represents the set of states and actions that connect them in the world. Search trees simply represents the path between these states that reaches the goal.
### 3.3.1 Best-first search
- Super general greedy approach.
- The next node $n$ is chosen from the evaluation function $f(n)$. The node on the frontier/fringe  corresponding to the minimum value is chosen for expansion.
	- Each child node is then added to the frontier if it has not been reached before. If reached before, it is now re-added if the path cost is less than previously.
### 3.3.2 Search data structures
- Search algorithms require a data structure to keep track of the search tree.
- The most basic data structure is a node, represented with 4 components:
	- node.state: the state to which the node corresponds.
	- node.parent: the node in the tree that generated this node.
	- node.action: the action that was applied to the parent's state to generate this node.
	- node.path-cost: the total cost of the path from the initial state to this node. $g(node)$ is a synonym for path-cost.
- We also need a data structure to store the frontier/fringe. Most of the time, the best is a queue. The frontier usually needs the following functions:
	- An empty check.
	- A pop (remove and returns).
	- A way to retrieve (not remove) the most recently added.
	- A way to add more elements.
- Three kinds of queues are used:
	- Priority queue: Elements are sorted by minimum cost, pop returns and removes the least cost element.
	- FIFO (First-in-First-out): Pop returns and removes the first element that was added to the queue. Usually used in BFS.
	- LIFO (Last-in-First-out): AKA a stack. Pop returns and removes the most recently added element. Usually used in DFS.
### 3.3.3 Redundant paths
- Repeated states are states that appear more than once in a path.
- Cycles (loopy path) result in repeated states.
	- Complete search trees of environments that allow loops are infinite.
- Eliminating redundant paths allows great search optimization.
	- "Algorithms that cannot remember the past are doomed to repeated it."
- Three approaches to the problem of redundant paths:
	- Remembering all previously reached states. Ideal if memory is large enough.
	- Graph searches check for redundant paths while tree-like searches don't. Tree-like searches will run slower but take up less memory.
	- A compromise of checking for cycles but not redundant paths in general. We can check for cycles by occasionally following back up the chain of states that a path has generated to check if the algorithm is stuck in a cycle. No need for additional memory and only takes a constant amount of time to check to remove all short cycles.
### 3.3.4 Measuring problem-solving performance
- An algorithm's performance can be evaluated in 4 ways:
	- Completeness: is it guaranteed to find a solution if it exists? If not, will it report a failure correctly?
		- Algorithms must be systematic in infinite state spaces. But it will search forever due to infinity.
	- Cost optimality
	- Time complexity
	- Space complexity
## 3.4 Uninformed Search Strategies
- Searches that have no clue about how close a state is to the goal(s).
### 3.4.1 Breadth-first search
- Steps:
	- The root node is expanded first.
	- All successor nodes are then expanded.
	- (Basically all nodes at the same level are expanded).
- Implemented with a FIFO queue.
- Good for when the goal is expected to be close to the root node.
- Always finds the solution with a minimal number of actions (not cost-optimal, unless all costs are equal to 1).
- Search and time complexity is $O(b^d)$ where b is the number of successors and d is the level.
- Terrible for problems with large search spaces.
![[Pasted image 20260211160849.png|500]]
### 3.4.2 Dijkstra's or uniform-cost search
- Complete and cost-optimal.
- Strategy:
	- Expand the cheapest node first.
	- Fringe is a priority queue.
		- All discovered nodes are added to the priority queue. That is, they are ordered in the queue based on the cost it takes to get to that node.
	- (Basically, at the root, pick the cheapest edge to go along. Repeat this until exhausted, then go back up to the most recent split and repeat.)
- STOP CONDITION:
	- Only when the goal node is popped from the priority queue.
		- This means the algo will continue to expand all other cheaper nodes.
		- Ex: If your optimal path is a cost of 50, UCS will process all nodes/paths that cost less than 50. Every other more expensive path will be ignored.
- Effective depth: $C^{*}/\epsilon$, for C* being the cost of the optimal solution, and $\epsilon$ being arc cost (total directed edge cost).
- Time complexity: $O(b^{\frac{C^{*}}{\epsilon}})$ 
- Fringe storage: $O(b^{\frac{C^{*}}{\epsilon}})$ 
### 3.4.3 Depth-first search and the problem of memory
- Always expands the deepest node in the frontier first.
- Not cost-optimal - always returns the first solution it finds.
- In-complete since it can get stuck in cycles in infinite state spaces.
- Advantages in requiring much less memory.
- Takes $O(b^m)$ search time and $O(bm)$ memory.
### 3.4.4 Depth-limited and iterative deepening search
- Prevents DFS from wandering down an infinite path. Uses a hyperparameter to restrict the depth of search.
- Depth-limited is not complete nor optimal.
- A poor setting will prevent the algorithm from reaching the solution, making it incomplete.
	- A good limit can sometimes be derived from the problem. 
		- Ex: Romania has 20 cities, so a limit of 19 is valid. But, further analysis reveals that any city can be reached from any other city in a most 9 actions. Thus a limit of 9 is much more effective.
- Iterative deepening search automates the limit selection process by trying all values: 0, 1, 2, 3, ... and so on. It is complete and optimal.
- Same requirements as DFS: 
	- memory = $O(bd)$ if a solution exists, or $O(bm)$ with no solutions.
	- time = $O(b^d)$ if a solution exists, or $O(b^m)$ with no solutions.
![[Pasted image 20260211162541.png|500]]
### 3.4.5 Bidirectional search
- Using two searches at the same time, one starting from the initial state and one from the goal state.
	- Need to keep track of two frontiers and two tables of reached states.
	- Need to also make sure we have a proper successor function for the reverse search.
- Is complete and optimal.
- Time: $O(b^{d/2})$
- Space: $O(b^{d/2})$
### 3.4.6 Comparing uninformed search algorithms
![[Pasted image 20260211203122.png]]
## 3.5 Informed (Heuristic) Search Strategies
- These searches use domain-specific hints about goal locations. These can find solutions more efficiently.
- These hints come from the heuristic function, $h(n)$.
### 3.5.1 Greedy best-first search
- This one expands the node with the lowest h(n) value, which is the node that appears closest to the goal.
- Complete in finite state spaces, but incomplete in infinite state spaces.
- Time and space complexity is $O(|V|)$.
	- A good $h(n)$ can reduce it down to $O(bm)$.
### 3.5.2 A* search
- Uses evaluation function $f(n) = g(n) + h(n)$.
	- $g(n)$ is path cost from initial state to current node $n$.
	- $h(n)$ is the estimated cost of the shortest path from $n$ to a goal state.
- A* uses a priority queue with values of $f(n)$.
### 3.5.3 Search contours
- If C* is the cost of the optimal solution path, then:
	- A* will expand all nodes that can be reached from the initial state on a path where every node on the path has $f(n) < C^*$. These are called surely expanded nodes.
	- A* might expand some of the nodes right on the contour where $f(n) = C^*$ before selecting a goal node.
	- A* will expand no nodes with $f(n) > C^*$.
- A* with a consistent heuristic is then called "optimally efficient" where it expands all nodes that are considered surely expanded nodes.
	- Efficient since it prunes away nodes that are not necessary for finding an optimal solution.
### 3.5.4 Satisficing search: Inadmissible heuristics and weighted A*
