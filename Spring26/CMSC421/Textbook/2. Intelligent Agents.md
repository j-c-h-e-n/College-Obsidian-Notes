- Rational agents are central to our approach to AI.
- Chapter 2 will refine and dive into the concepts of rational agents.
- Exploring the agents, environments, and the coupling between these two.
	- How agents behave depending on the environment.
	- How the types of environments influence agents differently.
- Introduce basic archetypes of agent designs.
## 2.1 Agents and Environments
>An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. 
- A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so on for actuators.
- A robotic agent may have cameras and infrared range finders for sensors and various motors for actuators.
- A software agent receives file contents, network packets, and human input as sensory inputs and acts on the environment by writing files, sending network packets, and displaying information or generating sounds.
	- The environment could be anything and everything!
![[Pasted image 20260203022954.png|300]]
- Percept: refers to the content an agent's sensors are perceiving.
	- Percept sequence: the complete history of everything the agent has ever perceived.
- An agent's behavior is described by the *agent function* that maps any given percept sequence to an action.
## 2.2 Good Behavior: The Concept of Rationality
- From an agent perspective: What does it mean to do the right thing?
### 2.2.1 Performance measures
- AI has generally stuck to consequentialism - we evaluate an agent's behavior by its consequences.
- Agent thrown into environment $\rightarrow$ generate sequence of actions from percepts $\rightarrow$ perform actions/change environment $\rightarrow$ we evaluate the sequence of environment changes. 
	- If this sequence was desirable, then it has performed well. Desirability is defined by a performance measure that evaluates any given sequence of environment states.
- Really difficult to formulate a proper performance measure.
	- General rule: it is better to design performance measures according to what one actually wants to be achieve int he environment, rather than according to how one thinks the agent should behave.
- We need to build agents that reflect initial uncertainty about the true performance measure, and have them self-learn about it as time goes by at the environments of different clients.
### 2.2.2 Rationality
- Rationality is based on these 4 things:
	- The performance measure that defines the criterion of success.
	- The agent's prior knowledge of the environment.
	- The actions that the agent can perform.
	- The agent's percept sequence to date.
- Rational agent: For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.
### 2.2.3 Omniscience, learning, and autonomy
- Need to distinguish between rationality and omniscience.
	- Omniscient agents know the actual outcome of its actions and can act accordingly.
		- But impossible in reality - no way to fully account for all outcomes for all actions.
	- Rationality != perfection, it simply maximizes expected performance. Omniscience/perfection maximizes actual performance.
- Information gathering (i.e. looking both ways before crossing the street) is an important part of rationality, to maximize expected performance (by maximizing the chances their future percepts will be advantageous/successful).
	- Another form of information gathering is exploration that must happen in unfamiliar environments.
- Rational agents should be autonomous, being able to learn from each percept, augmenting their own partial/incorrect prior knowledge.
	- ex. a vacuum robot that, overtime, learns and predicts tiles that become dirty.
- Thus, agents need to be designed with basic reflexes and knowledge, and be allowed to learn when they are deployed.
## 2.3 The Nature of Environments
- Task environments are the problem as to rational agents are the solution.
- Task environments directly affect the appropriate design for the agent program.
### 2.3.1 Specifying the task environment
- Performance measures, environment, and the agent's actuators and sensors are all encapsulated under the heading of task environment.
- The acronym is PEAS (performance, environment, actuators, sensors).
- The first step in designing an agent is to specify the task environment as CLEARLY as possible.
![[Pasted image 20260204161918.png]]

- Performance measure: Some goals here will conflict, so tradeoffs will be required.
- Environment: The more restricted the environment, the easier the design problem. Removes the concerns of an open-ended problem.
- Actuators: The tools available to the agent.
- Sensors: How the agent perceives its environment.
### 2.3.2 Properties of task environments
#### FULLY OBSERVABLE VS PARTIALLY OBSERVABLE
- We can divide the entirety of task environments into two labels: fully observable and partially observable.
- Fully observable:
	- Sensors detect all aspects of the environment that are relevant to the choice of action. Relevance defined with respect to the performance measure.
	- Nice since the agent does not need an internal model of the world - it's all readily existing and accessible.
- Partially observable: 
	- Information about the environment is obfuscated for any reason.
	- Dirty/noisy/inaccurate sensors may result in a partially observable world.
- Unobservable:
	- Usually not the case, but this is if the agent has no sensors at all. Still possible to achieve goals but definitely much harder.
#### SINGLE-AGENT VS MULTIAGENT
- Entities must be viewed as an agent if they are best described as maximizing a performance measure whose value depends on agent A's behavior.
	- i.e. another player in a competitive game, mobs in Minecraft, etc.
- Chess is a competitive multiagent environment since entity A and B are both trying to maximize their own performance at the cost of the other.
- Taxi-driving is a partially cooperative multiagent environment since it is in all agent's favor to not have collisions with one another. Also partially competitive due limited customers.
- Multiagent environments may result in the emergence of communication in collaborative environments, or randomized behavior in competitive environments.
#### DETERMINISTIC VS NONDETERMINISTIC
- If the next state of the environment is completely determined by the current state and the action from the agent then the environment is deterministic.
	- Otherwise, it is nondeterministic - external factors outside of the agent's actions influence the environment.
- Partially observable environments could appear to be nondeterministic due to unknowns.
- Many real situations have far too many factors, often treated as nondeterministic.
- Sometimes stochastic is a synonym for nondeterministic. In this textbook, stochastic refers to explicit dealings with probabilities. Nondeterministic if the probabilities are not quantified.
#### EPISODIC VS SEQUENTIAL
- Episodic task environments:
	- Future agent states do not depend on previous actions. Another way: current decisions/actions do not affect future states.
		- ex: robot on an assembly line dictating whether the part in front of them is good or bad.
	- Much simpler, agents do not need to think ahead/evaluate consequences.
- Sequential task environments:
	- Current decisions affect future states (and therefore decisions).
		- ex: Chess where your current move influences how the game plays out in subsequent steps/states.
#### STATIC VS DYNAMIC
- If the environment is actively changing while an agent is deliberating their next action, then the environment is dynamic. Otherwise, it is static (the environment can change during any moment EXCEPT during deliberation).
- The environment is called semi-dynamic if the environment itself doesn't change, but the agent's performance score does while deliberating (penalizes inefficient deliberation).
- Taxi driving is dynamic. Chess is semi-dynamic (with a clock). Crossword puzzles are static.
#### DISCRETE VS CONTINUOUS
- Discrete vs continuous are labels applied to the state of the environment, time, percepts, and actions of the agent.
- ex:
	- Chess has a finite number of distinct states (excluding the clock) and a discrete set of percepts and actions.
	- Taxi driving is a continuous state and continuous time problem where the speed and location of the taxi as well as other vehicles sweep through a continuous range of values smoothly over time. Driving actions are also continuous (throttle position, steering angle, etc).
#### KNOWN VS UNKNOWN
- This label refers to the knowledge of every outcome from every action (the "rules" of the environment).
- An environment is known if the agent/designer's knowledge encompasses all outcomes for all actions.
	- Known environments can be partially observable: a game of solitaire where you know all the rules and outcomes, but you don't know the cards that are turned down.
- If the environment is unknown, then the agent will have to learn how it works to make good decisions.
	- Unknown environments can fully observable - a video game can show the entire game state but the controls are still unknown to you.
## 2.4 The Structure of Agents
- The job of AI is to design an agent program that implements the agent function (mapping of percepts to actions).
- The agent program will be running on some computing device with sensors and actuators. The computing device is the agent architecture.
	- Agent as a whole = architecture + program.
- We focus on the program.
### 2.4.1 Agent programs
- Programs described in this book will all have the same skeleton: percept in $\rightarrow$ action out.
- Notice: agent program takes current percept as input while the agent function may depend on the entire percept history.
- To make a rational agent with this idea of program, then we can construct a table of all percept-action relations (Table-Driven-Agent).
	- But this form is incredibly inefficient. Let $P$ be the set of possible percepts and $T$ be the lifetime of the agent. As a result, the lookup table will contain $\sum^{T}_{t=1} |P|^T$ entries. A lookup table for chess has $10^{150}$ entries.
	- We will (a) reach unrealistic storage requirements, (b) designers cannot possibly create such a table in a timely manner, (c) no agent could ever learn all the right table entries from experience.
	- **DESPITE** all this, a Table-Drive-Agent will successfully do what we want. The implementation is simply unrealistic.
> The key challenge for AI is to find out how to write programs that, to the extent possible, produce rational behavior from a smallish program rather than from a vast table.
- There are 4 basic kinds of agent programs that embody the principles underlying almost all intelligent systems:
	- Simple reflex agents.
	- Model-based reflex agents.
	- Goal-based agents.
	- Utility-based agents.
### 2.4.2 Simple reflex agents
