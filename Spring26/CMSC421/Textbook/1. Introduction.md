- The field of AI "is concerned with not just understanding but also *building* intelligent entities - machines that can compute how to act effectively and safely in a wide variety of novel situations".
## 1.1 What Is AI?
- "Some AI systems use machine learning methods to achieve competence, but some do not".
- Research in two dimensions: human vs rational and thought vs behavior.
### 1.1.1 Acting humanly: The Turing test approach
- To pass the Turing test (Alan Turing 1950), a computer had to have:
	- NLP to communicate successfully in a human language.
	- Knowledge representation to store what it knows or hears.
	- Automated reasoning to answer questions and to draw new conclusions.
	- Machine learning to adapt to new circumstances and to detect and extrapolate patterns.
- Other researchers proposed a **total Turing test**. Adding on two more requirements:
	- Computer vision and speech recognition to perceive the world.
	- Robotics to manipulate objects and move about.
### 1.1.2 Thinking humanly: The cognitive modeling approach
- Three ways to learn about human thought:
	- Introspection: trying to catch our own thoughts as they go by.
	- Psychological experiments: observing a person in action.
	- Brain imaging: observing the brain in action.
- After sufficient data, we can attempt to express our theories of the brain as a computer program.
- Neurological research and AI develop separately but in parallel, these two fields can shed further light in each other.
- Cognitive modeling approach.
### 1.1.3 Thinking rationally: The "laws of thought" approach
- Logicians in 19th century developed precise notation for statements about objects in the world and the relations between them.
- By 1965 programs could solve any *solvable* problem in logical notation.
- Logic conventionally requires knowledge that is **certain**. But the real world is not this way, the *theory of probability* serves to fill this gap, allowing logic with uncertain information.   
- This allows the generation of rational thought, but not behavior.
- Logics approach.
### 1.1.4 Acting rationally: The rational agent approach
- Agent: Something that acts.
- All computer programs do something, but computer *agents are expected to operate autonomously*. i.e. perceive their environment, persist over a prolonged time period, adapt to change, and create and pursue goals.
- A rational agent aims to achieve the best or best expected outcome.
	- Determine best action, then execute.
	- OR, act on reflex that does not need inference (recoiling from hot stove).
- All skills needed for the Turing test also fulfills the requirements for an Agent to be considered rational.
- Advantages over cognitive and and logics approach:
	- More general than the logics approach.
	- More amenable to scientific development due to the standard of rationality being mathematically well defined and completely general.
- Early rational agents were built on deterministic situations. Later agents based on probability theory and ML allowed decisions under uncertainty.
- The standard model: The "right thing" to do/achieve defined by the objective we provide to the agent.
### 1.1.5 Beneficial machines
- The standard model isn't the best for the long run since it assumes we have a fully specified objective.
- Standard model works with:
	- Chess.
	- Shortest-path.
- Doesn't work with:
	- Autonomous driving where anything can happen between destinations.
	- Human-robot interaction intersections.
- Value Alignment Problem: The trouble of achieving agreement between our human true preferences and the objective we feed to the machine. The objectives we provide must be aligned with those of the human.
	- Systems deployed in the real world with the incorrect, non-human aligned objectives will have negative consequences.
	- Smart chess machines may go beyond the chess board - attempts to hypnotizing, blackmailing, etc. opponents to achieve victory no matter what.
	- Attempts to use the standard model in more advanced machines will result in too many loose ends. We need machines to pursue *our* objectives, not *their* objectives.

## 1.2 Foundations of AI
- Brief history of disciplines that contributed ideas, viewpoints, and techniques to AI.
### 1.2.1 Philosophy
- Questions to consider:
	- Can formal rules be used to draw valid conclusions?
	- How does the mind arise from a physical brain?
	- Where does knowledge come from?
	- How does knowledge lead to action?
- Dualism: "If the mind were entirely governed by physical laws, then there would be no room for free will". Claimed animals were not subject, therefore could be treated as machines as opposed to the free willed humans.
- Alternative to dualism is materialism, state that the mind is the result of the brain's operation under the laws of physics.
	- Physicalism and naturalism are synonyms to materialism.
- Now, modern AI adopt the approach of deontological ethics, rule-based, which highlights the idea of "doing the right thing" as not classified by the outcomes, but by universal social laws that govern allowable actions (i.e. not killing, not lying, etc).
### 1.2.2 Mathematics
- Questions to consider:
	- What are the formal rules to draw valid conclusions?
	- What can be computed?
	- How do we reason with uncertain information?
- Formal logic is formed on the basis of propositional/Boolean logic. Then expanded with objects and relations, resulting in the first-order logic used today. (think of the logical statements learned in Discrete Math, and often used in proof language).
- Theory of probability dealt with the computation of uncertain situations (gambling, percentages, probabilities) that became a way to deal with uncertain measurements and incomplete theories. Jacob Bernoulli, Pierre Laplace, Thomas Bayes, etc.
	- Baye's rule is critical to AI.
- The first non-trivial algorithm is thought to be Euclid's alg for GCD.
- Tractability:
	- A problem is intractable if the time required to solve instances of the problem grows exponentially with the size of the instances.
	- Exponential and polynomial growth was emphasized where moderately large instances of problems with exponential growth cannot be solved in any reasonable time.
- NP-Completeness (Non-deterministic Polynomial):
	- Any problem class to which the class of NP-complete problems can be reduced is likely to be intractable.
	- Problems that are solvable in exponential time and verifiable in polynomial time.
	- It has not been proved that NP-complete problems are necessarily intractable.
### 1.2.3 Economics
- Questions to consider:
	- How should we make decisions in accordance with our preferences?
	- How should we do this when others may not go along?
	- How should we do this when the payoff may be far in the future?
- Decision theory combined probability theory with utility theory, providing a formal and complete framework for individual decisions (economic or otherwise) made under uncertainty.
	- Suitable for "large" economies where each agent don't need to pay attention to the actions of other agents as individuals.
	- "You vs nature/uncertainty".
- Game theory introduced a surprising result that sometimes a rational agent should adopt policies with randomization.
- Unlike decision theory, game theory doesn't offer an unambiguous prescription for selection actions.
	- "You vs other players".
- In AI, decisions involving multiple agents are studied under the heading of multiagent systems.
- Sequential decision problems (where payoffs aren't reaped immediately, but rather in the future after several turns) are modeled with Markov Decision Processes.
- There has been a resurgence of making models that make "good enough" decisions over the optimal decision in favor of a more human-like decision process, as well as compute efficiency.
### 1.2.4 Neuroscience
-  Question:
	- How do brains process information?
- Neuroscience studies the nervous system, particularly the brain.
- The development of brain-machine interfaces allow the recovery of sensing and motor controls to patients while also accelerating understanding of our neural systems.
- Although computers have been accelerating at an exceptional rate, they still do not rival natural human brains. Ultimately, without the right theory, faster machines just give you the wrong answer faster.
### 1.2.5 Psychology
- Question:
	- How do humans and animals think and act?
- Behaviorism studied ONLY the theories that could be readily observed from the behaviors of subjects. A lot was discovered from rats and pigeons, but was less successful with understanding humans.
- Cognitive psychology viewers the brain as an information-processing device.
	- Cognitive psychology was shadowed under behaviorism until the re-establishment of the legitimacy of "mental" terms such as beliefs and goals.
- Human-computer interaction can be counted under psychology with one of the pioneers, Doug Engelbart, championed the idea of "intelligence augmentation".
### 1.2.6 Computer engineering
- Question:
	- How can we build an efficient computer?
- Moore's law: estimated that computer performance would double every 18 months or so, until it ended in 2005. Since, Moore's law as been reflective of CPU cores. But now we are hitting the limits of that too.
- Specialized hardware for AI applications began arising with the GPU, TPU (tensor processing unit), and WSE (wafer scale engine).
- ML training speeds exploded from 2012 to 2018 and quantum computing seems to hold greater promises for certain subclasses of AI algorithms in the future.
### 1.2.7 Control theory and cybernetics
- Question:
	- How can artifacts operate under their own controls?
- Control theory: derived from math and engineering, modelling dynamic systems and algorithms to create the desired behavior out of a machine.
- Modern control theory, especially stochastic optimal control, designs systems that maximize a cost function over time (i.e. achieving optimality).
- Calculus and matrices are the tools of control theory.
### 1.2.8 Linguistics
- Question
	- How does language relate to thought?
- The behaviorist theory did not address the notion of creativity in language (how children could understand and make up sentences that they had never heard before).
- AI and linguistics intersect in a field called NLP (natural language processing).
- Understanding language is not just understanding structures of sentences, but also subject matter and context.
## 1.3 The History of Artificial Intelligence
- The list of Turing Award winners is an effective way to summarize the milestones of AI.
### 1.3.1 The inception of artificial intelligence (1943 - 1956)
- Hebbian Learning is a simple rule for modifying connection strengths between neurons (nodes in a mathematical network model).
- The first neural network was created in 1950, it was a computer made of 3000 vacuum tubes and mechanisms from a B-24 bomber to simulate a 40 neuron network.
- Alan Turing in 1950 introduced the Turing test, machine learning, genetic algorithms, and reinforcement learning.
### 1.3.2 Early enthusiasm, great expectations (1952 - 1969)
- Intellectual establishment largely preferred to believe that a machine could never do $X$.
- AI researchers demonstrated one $X$ after another. Period referred by John McCarthy as "Look, Ma, no hands!".
- The General Problem Solver (GPS) was created, designed to imitate human problem-solving protocols from the ground up. Probably the first program to embody the "thinking humanly" approach.
	- The inventors Newell and Simon followed up with the physical symbol system hypothesis: Any system exhibiting intelligence must operate by manipulating data structures composed of symbols.
- Arthur Samuel worked with methods that we now call reinforcement learning, playing checkers at a strong amateur level.
	- The computer could then play checkers better than him.
	- TD-Gammon and AlphaGo were successors
- In 1958, John McCarthy created Lisp, which became the dominant AI programming language for the next 30 years.
	- He also proposes the idea of a new invention called the "Advice Taker" which could accept new axioms in the normal course of operation, allowing it to learn without being reprogrammed (an evolving agent). This paper influences the course of AI today.
- 1963 Saint, 1968 Analogy, and 1967 Student programs were sufficient at limited domains (microworlds), showing the specialization required at early attempts of artificial intelligence.
	- Most famous microworld was "Block World" which resulted in work on learning theory, NLP, vision processing, and planners.
### 1.3.3 A dose of reality (1966 - 1973)
- Early efforts were plagued with researcher overconfidence, estimating that computers would become chess champions and gain significant intelligence within 10 years. However it has taken 40.
- Early AI systems were flawed, they were based on "informed introspection" on how humans perform a task, rather than the careful analysis of the task, what it means to be a solution, and what an algo would need to do to reliably produce such solutions.
	- Microworlds also had limited scope and facts, the early systems fell apart when more variables were thrown into problems.
- Perceptrons were created but were thought to be not useful as they couldn't represent very much. Neural network research dried up until back-propagation algorithms brought back a resurgence in NN research in the late 1980s and again in the 2010s.
### 1.3.4 Expert systems (1969 - 1986)
- Early AI used approaches that have been called "weak methods" since although they are general, they did not scale up to large or difficult problem instances.
- Later systems attempted to use a method where the solution was already known, and the programs would then solve for the correct answer. These were used in narrow areas of expertise.
- Naive programs such as Dendral for molecular structures would general all possible structures consistent with a provided formula, then chose the best one. This is intractable for even moderately sized molecules.
- Next was Mycin for diagnosing blood infections. Contained 450 rules and was able to perform as well as some experts. Was considerably better than junior doctors. Rules were acquired through extensive interviewing of experts and rules reflected the uncertainty associated with medical knowledge.
- Efforts to understand natural language took place.
- AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988.
- Promises were made but companies failed to deliver on extravagant promises due to the systems breaking down in face of uncertainty and their inability to learn from experience.
### 1.3.5 The return of neural networks (1986 - present)
- Back-propagation was reinvented in the mid 1980s.
- Emphasis placed again on symbols, citing that at the basic level, humans manipulate symbols.
- Symbols were described as the "luminiferous aether of AI".
- Back-propagation allowed these systems to learn from examples, comparing predicted outputs to the true values on a problem, then re-learning through back-propagation to tweak parameters, allowing better future performance.
### 1.3.6 Probabilistic reasoning and machine learning (1987 - present)
- The fragile nature of expert systems lead to more scientific approaches that were based on probability rather than Boolean logic.
- This led to dependence on machine learning over hand coding (hard coding).
	- Also experimental results rather than philosophical claims.
- A new focus on real world applications and reconstruction on solid existing theories.
- A creation of shared problem sets became the norm for demonstrating progress such as the LibriSpeech corpus for speech recognition, the MNIST data set for handwritten digits, ImageNet and COCO for object recognition, SQuAD for natural language question answering, the WMT competition for machine translation, and the International SAT Competitions for Boolean satisfiability solvers.
- AI has resulted in newfound appreciation for data, statistical modeling, optimization, and ML, which in turn also reunified subfields such as computer vision, ML, multiagent systems, and NLP.
### 1.3.7 Big data (2001 - present)
- Advances in compute and the creation of the WWW facilitated the creation of super large data sets.
- Lead to development of algorithms specially designed to leverage these data sets.
- Significant amount are unlabeled data. But with enough data and epochs, learning algorithms can achieve accuracy.
- Banko and Brill (2001) argued that improvement in performance obtained from increasing the data set size by two or three orders of magnitude outweighs any improvement that can be obtained from tweaking the algorithm.
### 1.3.8 Deep Learning (2011 - present)
- Deep learning refers to ML using multiple layers of simple, adjustable computing elements.
- Convolutional neural networks were used to successfully recognize handwritten digits.
- Deep learning really took off in 2011 with speech and visual object recognition.
- Exceeded expectations and continues to impress.
- Heavily relies on powerful hardware. A standard CPU can do $10^9$ to $10^{10}$ operations per second, but a DL algo running on specialized hardware (GPU/TPU/FPGA) might achieve $10^{14}$ to $10^{17}$ operations per second in the forms of highly parallelized matrix and vector operations.
## 1.4 The State of the Art
- Highlights from reports that tracked between 2000 and 2019 (unless otherwise stated):
	- Publications: AI papers increased 20-fold between 2010 and 2019 to about 20,000 a year.
	- The most popular category was machine learning. (Machine learning papers in arXiv.org doubled every year from 2009 to 2017.) Computer vision and natural language processing were the next most popular.
	- Sentiment: About 70% of news articles on AI are neutral, but articles with positive tone increased from 12% in 2016 to 30% in 2018. The most common issues are ethical: data privacy and algorithm bias.
	- Students: Course enrollment increased 5-fold in the U.S. and 16-fold internationally from a 2010 baseline. AI is the most popular specialization in Computer Science.
	- Diversity: AI Professors worldwide are about 80% male, 20% female. Similar numbers hold for Ph.D. students and industry hires.
	- Conferences: Attendance at NeurIPS increased 800% since 2012 to 13,500 attendees. Other conferences are seeing annual growth of about 30%.
	- Industry: AI startups in the U.S. increased 20-fold to over 800.
	- Internationalization: China publishes more papers per year than the U.S. and about as many as all of Europe. However, in citation-weighted impact, U.S. authors are 50% ahead of Chinese authors. Singapore, Brazil, Australia, Canada, and India are the fastest growing countries in terms of the number of AI hires.
	- Vision: Error rates for object detection (as achieved in LSVRC, the Large-Scale Visual Recognition Challenge) improved from 28% in 2010 to 2% in 2017, exceeding human performance. Accuracy on open-ended visual question answering (VQA) improved from 55% to 68% since 2015, but lags behind human performance at 83%.
	- Speed: Training time for the image recognition task dropped by a factor of 100 in just the past two years. The amount of computing power used in top AI applications is doubling every 3.4 months.
	- Language: Accuracy on question answering, as measured by F1 score on the Stanford Question Answering Dataset (SQUAD), increased from 60 to 95 from 2015 to 2019; on the SQUAD 2 variant, progress was faster, going from 62 to 90 in just one year. Both scores exceed human-level performance.
	- Human benchmarks: By 2019, AI systems had reportedly met or exceeded human-level performance in chess, Go, poker, Pac-Man, Jeopardy!, ImageNet object detection, speech recognition in a limited domain, Chinese-to-English translation in a restricted domain, Quake III, Dota 2, StarCraft II, various Atari games, skin cancer detection, prostate cancer detection, protein folding, and diabetic retinopathy diagnosis.
- Today, AI can be relevant to:
	- Robotic vehicles
	- Autonomous planning and scheduling
	- Machine translation
	- Speech recognition
	- Recommendations
	- Image understanding
	- Medicine
	- Climate science
## 1.5 Risks and Benefits of AI
- Some risks are already apparent, while others seem likely to happened based on future projections:
	- Lethal autonomous weapons.
	- Surveillance and persuasion.
	- Biased decision making.
	- Impact on employment.
	- Safety-critical applications.
	- Cybersecurity.
