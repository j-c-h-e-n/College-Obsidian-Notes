If you're not doing supervised learning, it's likely a timeseries.
LOOKS VERY SIMILAR TO A REGRESSION PROBLEM.

- Continuous
- Causal impact
- Next point is dependent on the previous point (definition of timeseries).

- In data science, we consider a series to be a time series if past observations contain predictive value for the future (does not necessarily have to have time as an axis).
- examples:
	- Stock market,
	- Temperature over a month.
	- Population over time.
	- Windspeed.
## Features
- Typically we start extracting features in a meaningful way.
	- We don't care for timeseries.
- Super simple (t, y), some time and some output.
- Just plot it and stare at it.
## We may want to:
- Predict next values.
- Perform some statistical analysis on the formula/model we create.
- Compress the time series so we can categorize it.
## Plotting
- We may notice the following when plotting a timeseries:
	- Trend: upward or downward pattern that might be extrapolated into the future (overall direction).
	- Periodicity: Repetition of behavior in a regular pattern.
	- Seasonality: Periodic behavior with a known period (hourly, monthly, etc).
	- Heteroskedasticity: Changing variance.
	- Dependence: Positive (successive observations are similar) or negative (successive observations are dissimilar).
## Predicting Missing Data
- Just use linear imputation. 
	- Draw a line between the cut off data points.
## Noise
- Sometimes, observations are randomly shifted away from trend.
- Noise can be generated by any distribution, but it is usually gaussian in nature.
## Dicky-Fuller Test
- Tests for stationarity in a timeseries:
	- A random variable or random process is said to be stationary if all of its statistical parameters are independent of time
- Tests if there is a root unit in the timeseries. If there isn't, it is stationary.
- $H_0$: A unit root is present in the timeseries (it is not stationary).
- $H_1$: A unit root isn't present.

