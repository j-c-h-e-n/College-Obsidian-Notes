  ## Complex Node Architectures
From Perlmutter @ NERSC
![[Pasted image 20241031153743.png|300]]
![[Pasted image 20241031153756.png|300]]
![[Pasted image 20241031153810.png]]
## Several Possible Approaches
- Use MPI everywhere:
	- Ex: 2 nodes with 128 nodes each, create 256 MPI processes.
- Use MPI+X where X handles within node parallelization.
	- MPI handles inter-node communication.
	- Also referred to as hybrid programming.
- X could be OpenMP for CPUs and CUDA for GPUs.
	- CPU nodes: Create 1 MPI process and 128 threads per node.
	- GPU nodes: Create 1 MPI process per GPU and use CUDA for launching GPU kernels.
# Hybrid Programming
- We use hybrid programming because:
	- Processes are heavy-weight.
	- Using MPI everywhere can lead to a large number of messages.
	- Using threads can enable better sharing of data on symmetric multi-processing (SMP) and multi-core nodes.
	- Larger grain size (per MPI process) can help with fewer overheads.
	- Required when you have GPUs attached to a node.
- Additional choices for X:
	- CPUs: OpenMP, pthreads, RAJA, Kokkos, ...
	- GPUs: CUDA, HIP, OpenMP offload, RAJA, Kokkos, ...
	- Some models can be used on both CPUs and GPUs:
		- Called "portable" programming models.
# MPI+OpenMP
## 2D Stencil: MPI+OpenMP
![[Pasted image 20241031160457.png]]
## Different Methods for MPI Communication
- `MPI_THREAD_SINGLE`: All MPI communication is done by the main OpenMP thread *outside* of OpenMP regions.
- `MPI_THREAD_FUNNELED`: All MPI communication is done by the main OpenMP thread *inside* OpenMP regions
- `MPI_THREAD_SERIALIZED`: Multiple threads call MPI routines but one thread at a time.
- `MPI_THREAD_MULTIPLE`: Multiple threads call MPI routines, potentially simultaneously.
![[Pasted image 20241031163642.png]]
## Process and Thread Affinity
- The number of processes and threads depends.
- Normally, the OS can run processes and threads on any core, and even move them around.
- For performance, it's best to assign (pin) processes/threads to specific cores.
- Use slurm options such as --tasks-per-node and --cpus-per-task to spread tasks apart.
	- Assigning (pinning): --cpu-bind, OMP_PROC_BIND
# MPI+CUDA
- Typically, one MPI process manages one GPU. 1:1
```cpp
MPI_Comm_rank(icomm, &myrank);   // MPI rank

int deviceCount;
cudaGetDeviceCount(&deviceCount); // num GPUs

int device_id = myrank % deviceCount;
cudaSetDevice(device_id);   // Map MPI Process to GPU
```
- Send data to other nodes using the MPI processes on each node.
## Sending Data to other GPUs/Nodes
- Copy data from device to host and then send messages between MPI processes.
- GPU-aware MPI: You can provide GPU memory pointers in the MPI_Isend/MPI_Irecv calls.
	- Avoids the device to host memcpy in user code.
	- The runtime might still do a copy.
- MPI built with GPUDirect: When enabled, it avoids an extra copy and directly sends data between GPUs on different nodes.