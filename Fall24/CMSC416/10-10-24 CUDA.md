# GPGPUS (General Purpose GPUs)
## Used for Mainstream HPC
- 2013: NAMD, used for molecular dynamics simulations on a supercomputer with 3000 NVIDIA Tesla GPUs.
## Hardware
- Higher instruction throughput.
- Hide memory access latencies with computation.
![[Pasted image 20241014140643.png]]
## Volta GV100 SM
- Core Core
	- Single serial execution unit.
- Each Volta Streaming Multiprocessor (SM) has:
	- 64 FP32 cores.
	- 64 INT32 cores.
	- 32 FP64 cores.
	- 8 Tensor cores.
- CUDA capable device or GPU
	- Collection of SMs.
![[Pasted image 20241014140854.png|200]]
![[Pasted image 20241014141814.png]]
(example GPU, which is a large collection of SMs)
### GPU-Based Nodes
This shows a single node of the Summit supercomputer at ORNL.
![[Pasted image 20241014141943.png|300]]

# CUDA
A programming model for NVIDIA GPUs.
- Allowed developers to use C++ as a high-level programming language.
- Build around threads, blocks, and grids.
- Terminology:
	- Host: CPU
	- Device: GPU
	- CUDA kernel: a function that gets executed on the GPU.
## Abstraction of CUDA Software:
- Thread
	- Serial unit of execution.
	- ![[Pasted image 20241014142226.png|50]]
- Block
	- Collection of threads.
	- Number of threads in block <= 1024.
	- ![[Pasted image 20241014142456.png|50]]
- Grid
	- Collection of blocks.
	- ![[Pasted image 20241014142514.png|100]]
- Software is directly mapped to hardware:
- ![[Pasted image 20241014142603.png|500]]
## Steps to write a CUDA kernel
1. Copy input data from host (CPU) to device (GPU) memory.
2. Load the GPU program (kernel) and execute.
3. Copy results back to host memory.
## Copying Data from CPU to GPU
```cpp
double *d_Matrix, *h_Matrix;
h_Matrix = new double[N];

cudaMalloc(&d_Matrix, sizeof(double)*N);

// initialize h_Matrix
cudaMemcpy(d_Matrix, h_Matrix, sizeof(double)*N, cudaMemcpyHostToDevice);

// some computation on GPU
cudaMemcpy(h_Matrix, d_Matrix, sizeof(double)*N, cudaMemcpyDeviceToHost);

cudaFree(d_Matrix);
```
## CUDA Syntax
```cpp
__global__ void saxpy(float *x, float *y, float alpha) {
	int i = threadIdx.x;
	y[i] = alpha*x[i] + y[i];
}

int main() {
	...
	// <<<#blocks, threads_per_block>>>
	saxpy<<<1, N>>>(x, y, alpha);
	...
}
```
`saxpy`: "Single-Precision AÂ·X Plus Y". Function within the Basic Linear Algebra Subroutines (BLAS) library. 
- Does relatively little computation, not very useful for demonstrating difference in performance between different programming models.
## Compiling CUDA Code
```cpp
nvcc -o saxpy --generate-code arch=compute_80,code=sm_80 saxpy.cu
./saxpy
```
## Multiple Blocks
```cpp
__global void saxpy(float *x, float *y, float alpha, int N) {
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i < N)
		y[i] = alpha*x[i] + y[i];
}

int main() {
	...
	int threadsPerBlock = 512;
	int numBlocks = N/threadsPerBlock + (N % threadsPerBlock != 0);

	saxpy<<<numBlocks, threadsPerBlock>>>(x, y, alpha, N);
	...
}
```